{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xb0q8y0rq6Ve"
   },
   "source": [
    "# Nearest Neighbors Algorithm for Supervised Learning\n",
    "\n",
    "\n",
    "Nearest neighbors algorithms (NNAs) are very simple conceptually: to classify a datum with specific feature values, find the data point that has the most similar feature values and put the original datum in that class. NNAs can also be used to predict missing feature values. \n",
    "\n",
    "The most common NNA is the k-Nearest Neighbors algorithm where the top *K* nearest neighbors to the query are identified. In most instantiations of k-NNA, classification/prediction is based on a \"majority vote\" of the k nearest neighbors (ex. if k = 5 and 3 out of the 5 nearest neighbors are class A and 2 are class B, the new data point will be classified as A). In the image below, using k = 1 would yield a class 1 classification while k = 3 would yield class 2.\n",
    "\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/0*Sk18h9op6uK9EpT8.)\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "### A quick example to illustrate k-NNA:\n",
    "\n",
    "If we want to classify a car as \"cool\" or \"uncool\" using the features *horsepower, number of seats,* and *manual (0) or automatic (1)*, our dataset might look like this:\n",
    "\n",
    "*   150, 5, 0, uncool (2008 Honda Civic)  \n",
    "*   320, 5, 0, cool (2011 Dodge Charger)\n",
    "*   383, 3, 1, cool (1985 Chevy Blazer)\n",
    "*   210, 7, 0, uncool (2001 Honda Odyssey)\n",
    "\n",
    "Let's say we're trying to predict whether the 2017 Bugatti Veyron (1500hp, 2 seats, manual: 1) is cool or not. In the next cell the data is loaded. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1531158235401,
     "user": {
      "displayName": "Danelle Shah",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "100860318553187717982"
     },
     "user_tz": 240
    },
    "id": "a7rbFpWBrGb4",
    "outputId": "26f4325b-e439-4102-88bf-79145f5b4e41"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_dict = {'2008 Honda Civic':    {'hp':150., 'seats':5., 'auto':0., 'cool':0}, \n",
    "             '2011 Dodge Charger' : {'hp':320., 'seats':5., 'auto':0., 'cool':1}, \n",
    "             '1985 Chevy Blazer':   {'hp':383., 'seats':3., 'auto':1., 'cool':1}, \n",
    "             '2001 Honda Odyssey':  {'hp':210., 'seats':7., 'auto':0., 'cool':0}, \n",
    "             '2017 Bugatti Veyron': {'hp':1500.,'seats':2., 'auto':1., 'cool':None}}\n",
    "\n",
    "data = pd.DataFrame.from_dict(cars_dict,orient='index')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5STDuS1DF0PA"
   },
   "source": [
    "### Normalizing Data and Calculating Distance\n",
    "\n",
    "In order to tell how \"near\" one datum is to another, we need a way to measure the distance between two data. One of the simplest ways to do this (and the way we will be doing it) is with simple Euclidian distance (formula below). Euclidean distance is the square root of the sum of the difference between each feature squared:\n",
    "\n",
    "![alt text](https://i.stack.imgur.com/2y0bx.png.)\n",
    "\n",
    "Some other approaches include Chi square distance and cosine distance (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4978658/). \n",
    "\n",
    "NOTE: By normalizing the data in this way we are assuming all features are equally important, but there are ways to weight some features more/less than others.\n",
    "\n",
    "If we simply compute the Euclidean distance from one datum to another, values that are generally larger (like horsepower) will end up having a greater effect than the other smaller values. Because features with large values are not inherently more important for prediction than other values, we should normalize the data before we calculate distance. One quick and easy way to normalize data is to divide each datum by the maximum value in its category (i.e. divide each element in a row by the max value in that row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1531158434256,
     "user": {
      "displayName": "Danelle Shah",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "100860318553187717982"
     },
     "user_tz": 240
    },
    "id": "aBhWCPsFMn-C",
    "outputId": "b62e3aac-6ddf-4158-abf1-b7e23e4e9c75"
   },
   "outputs": [],
   "source": [
    "# Normalize the data by dividing each value by the maximum value in its row. \n",
    "# Do not normalize the labels indicating cool and uncool (row 3)\n",
    "\n",
    "for i in ['hp','seats','auto']:\n",
    "    ## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use sklearn's [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) method to standardize the data.\n",
    "\n",
    "Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual feature do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data by removing the mean and scaling to unit variance from each feature\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for i in ['hp','seats','auto']:\n",
    "    ## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a function that will calculate the euclidean distance between two data points (you can assume the data points will be arrays of the same length). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "2rayoFQnOyvY"
   },
   "outputs": [],
   "source": [
    "# Distance function using formula for euclidean distance\n",
    "\n",
    "def euclidean_dist(datum1, datum2):\n",
    "\n",
    "    ## your code here\n",
    "    \n",
    "    return(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6IaktG5_mUNf"
   },
   "source": [
    "In the cell below, calculate the distance between the Bugatti and each other car using euclidean_dist. Remember not to use row three (cool/uncool label) when computing distance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "rY0Wuzz9LZzu"
   },
   "outputs": [],
   "source": [
    "# FYI: This is how you can call a specific row by name and sub-select features\n",
    "bugatti = data.loc[\"2017 Bugatti Veyron\"][[\"hp\",\"seats\",\"auto\"]].values\n",
    "print(bugatti)\n",
    "\n",
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rHRn4t68nIbB"
   },
   "source": [
    "For the first version of normalization (dividing by the max), you should get the following distances (rounded to the thousandths palce):\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Blazer = 0.758\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Odyssey = 1.500\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Civic = 1.412\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Charger = 1.343\n",
    "\n",
    "For the second version of normalization (standard norm), ou should get the following distances (rounded to the thousandths palce):\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Blazer = 2.305\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Odyssey = 4.363\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Civic = 3.796\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Charger = 3.562\n",
    "\n",
    "Notice that (in this case) both normalization techniques yielded the same order of cars nearest-to-farthest.  This will not always be the case!\n",
    "\n",
    "Because the distance between the Bugatti and Blazer is the smallest, if k = 1, we would classify the Bugatti as cool. If k = 4 we would not be able to classify the Bugatti in either category using the \"majority vote\" technique unless we had in place a tiebreaker protocol. Generally speaking, larger values of *k* reduce noise, but also make the boundaries between classes less distinct. The best value of *k* will depend on your dataset and your prediction needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vo5iwH6soijh"
   },
   "source": [
    "# Predicting Diabetes in Pima Heritage Dataset\n",
    "\n",
    "Next we will see if we can use k-NNA to predict whether or not a patient has diabetes given some medical information about them. Load and view the data in the cell below. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 402,
     "status": "ok",
     "timestamp": 1531159718063,
     "user": {
      "displayName": "Danelle Shah",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "100860318553187717982"
     },
     "user_tz": 240
    },
    "id": "hQAjk52WQ1da",
    "outputId": "14b27fa3-9a09-4762-aa2e-bcb576400d6c"
   },
   "outputs": [],
   "source": [
    "# Data Loading and preprocessing\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "data = pd.read_csv(url, names=names)\n",
    "\n",
    "#  'preg': number of pregnancies  \n",
    "#  'plas': plasma glucose concentration \n",
    "#  'pres': blood pressure \n",
    "#  'skin': skin thickness\n",
    "#  'test': Insulin\n",
    "#  'mass': BMI\n",
    "#  'pedi': diabetes pedigree function\n",
    "#  'age': age\n",
    "#  'class': '0' means does not have diabetes and '1' means has diabetes\n",
    "\n",
    "\n",
    "# look at the first 10 rows\n",
    "data.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lO_knzAAe93n"
   },
   "source": [
    "Now, let's clearly define which columns will act as explanatory variables, and which column will be the target value, and split the dataset between your training data and testing data.  Let's try an 80-20 split and use sklearn's [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) method (set random_state = 0 so we get the same output each time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1531160088058,
     "user": {
      "displayName": "Danelle Shah",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "100860318553187717982"
     },
     "user_tz": 240
    },
    "id": "b8pRTCC3PatJ",
    "outputId": "47ceb8c1-e308-4974-ba9f-91587232c917"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Your code here\n",
    "\n",
    "x_training, x_testing, y_training, y_testing = ## Your code here\n",
    "\n",
    "print(x_training.shape, y_training.shape)\n",
    "print(x_testing.shape, y_testing.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's not forget to normalize the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzZQ1VhiPSJU"
   },
   "source": [
    "This predict method needs to compute the euclidean distance between the “new” observation and all the data points in the training set. Then, it assigns the corresponding label to the observation. Finally, it selects the K nearest ones and performs a \"majority vote.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "UCganwZfe9LA"
   },
   "outputs": [],
   "source": [
    "def predict(x_training, y_training, x_test_sample, k):\n",
    "    \n",
    "    ## Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JNA504XOvjvY"
   },
   "outputs": [],
   "source": [
    "def knn(x_training, y_training, x_testing, k):\n",
    "    \n",
    "    ## Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "iew5gW_AbnfN"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "predictions_slow = knn(x_training, y_training, x_testing, k=5)\n",
    "\n",
    "print('Took {} seconds'.format(time.time() - start))\n",
    "predictions_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7-2ZM8AGIe0S"
   },
   "source": [
    "# Using sklearn to speed up normalizing and distance finding\n",
    "\n",
    "Luckily for us, sklearn has some quick and easy functions for normalizing the data, finding Euclidean distances, training, and testing with k-NNA. Try k = 5 to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 13073
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1530526208018,
     "user": {
      "displayName": "Lyle Lalunio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "111684296322842137827"
     },
     "user_tz": 240
    },
    "id": "R0FsKkp_qw-x",
    "outputId": "39c27885-10d9-4f03-9bff-ac7c41d5dcb8"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create Model with k nearest neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train/fit model with training data\n",
    "knn.fit(x_training, y_training)\n",
    "\n",
    "# Make predictions on the test data using the fitted model\n",
    "start = time.time()\n",
    "predictions_fast = knn.predict(x_testing)\n",
    "\n",
    "print('Took {} seconds'.format(time.time() - start))\n",
    "predictions_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check sklearn's predictions and make sure they match yours.  Sklearn is faster, but you should get the same answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating classification performance\n",
    "Let's see how well our classifier did by looking at the confusion matrix and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F3dhm8i3Q6nh"
   },
   "source": [
    "You should get F1 = 0.659. Notice that this classifier is pretty good at classifying negative samples (compare TN to FP), but we are not very good at classifying positive samples (compare TP to FN).\n",
    "\n",
    "Spend a few minutes trying to increase the F1 score as much as you can by changing k and the features of the data you are using to predict values.\n",
    "\n",
    "* What set of features and values of k did you find to be the most optimal?\n",
    "* Why is choosing the right features so important for prediction accuracy? \n",
    "* What other model hyperparameters (other than k) might we be able to tune?\n",
    "* How might we use non-numerical data columns in our model (if we had any)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l_noD6IVArqt"
   },
   "source": [
    "# Pros and Cons of k-NNA\n",
    "\n",
    "## Pros\n",
    "* Non-parametric (can be used with data that does not fit a normal distribution)\n",
    "* Conceptually simple and relatively simple to instantiate\n",
    "* Little to no \"training\" time \n",
    "* A good starting point/baseline classifier \n",
    "\n",
    "## Cons\n",
    "* Slow \"testing\" phase compared to other predictors/classifiers \n",
    "* Degrades with high-dimension data (because there is less difference between closest and furthest neighbors)\n",
    "* Unclear how to handle non-numeric features\n",
    "* Doesn't handle data with skewed class distribution well (if one class is extremely dominant in the training data, it will dominate the \"voting majority\" for classifying new data)\n",
    "* Features that are not of the same scale can (should) be normalized, but this introduces another model hyperparameter (i.e., which normalization method is appropriate?)\n",
    "* Does not take into account feature correlation\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "From these exercises and the pros and cons listed above, here is when it is most useful to use k-NNA:\n",
    "* Datasets with many data points and few dimensions (but can become very slow as well)\n",
    "* Datasets that are non-parametric\n",
    "* When you want a quick and easy classifier that does not have to be optimal (perhaps to use as a baseline for other models)\n",
    "\n",
    "\n",
    "Lastly, there are many other types of NNAs and there are many other ways to instantiate the k-NNA. For example, we could have used a weighted voting system (nearer neighbors' votes carry more weight) instead of a majority voting system to classify. Additionally, we could have used a different distance measurement or a different error measurement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Nearest_Neighbors_Solutions.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
