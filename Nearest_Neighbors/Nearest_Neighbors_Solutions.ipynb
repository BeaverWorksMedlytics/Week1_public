{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xb0q8y0rq6Ve"
   },
   "source": [
    "# Nearest Neighbors Algorithm for Supervised Learning\n",
    "\n",
    "\n",
    "Nearest neighbors algorithms (NNAs) are very simple conceptually: to classify a datum with specific feature values, find the data point that has the most similar feature values and put the original datum in that class. NNAs can also be used to predict missing feature values. \n",
    "\n",
    "The most common NNA is the k-Nearest Neighbors algorithm where the top *K* nearest neighbors to the query are identified. In most instantiations of k-NNA, classification/prediction is based on a \"majority vote\" of the k nearest neighbors (ex. if k = 5 and 3 out of the 5 nearest neighbors are class A and 2 are class B, the new data point will be classified as A). In the image below, using k = 1 would yield a class 1 classification while k = 3 would yield class 2.\n",
    "\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/0*Sk18h9op6uK9EpT8.)\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "### A quick example to illustrate k-NNA:\n",
    "\n",
    "If we want to classify a car as \"cool\" or \"uncool\" using the features *horsepower, number of seats,* and *manual (0) or automatic (1)*, our dataset might look like this:\n",
    "\n",
    "*   150, 5, 0, uncool (2008 Honda Civic)  \n",
    "*   320, 5, 0, cool (2011 Dodge Charger)\n",
    "*   383, 3, 1, cool (1985 Chevy Blazer)\n",
    "*   210, 7, 0, uncool (2001 Honda Odyssey)\n",
    "\n",
    "Let's say we're trying to predict whether the 2017 Bugatti Veyron (1500hp, 2 seats, manual: 1) is cool or not. In the next cell the data is loaded. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1531158235401,
     "user": {
      "displayName": "Danelle Shah",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "100860318553187717982"
     },
     "user_tz": 240
    },
    "id": "a7rbFpWBrGb4",
    "outputId": "26f4325b-e439-4102-88bf-79145f5b4e41"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hp</th>\n",
       "      <th>seats</th>\n",
       "      <th>auto</th>\n",
       "      <th>cool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1985 Chevy Blazer</th>\n",
       "      <td>383.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001 Honda Odyssey</th>\n",
       "      <td>210.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008 Honda Civic</th>\n",
       "      <td>150.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011 Dodge Charger</th>\n",
       "      <td>320.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017 Bugatti Veyron</th>\n",
       "      <td>1500.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         hp  seats  auto  cool\n",
       "1985 Chevy Blazer     383.0    3.0   1.0   1.0\n",
       "2001 Honda Odyssey    210.0    7.0   0.0   0.0\n",
       "2008 Honda Civic      150.0    5.0   0.0   0.0\n",
       "2011 Dodge Charger    320.0    5.0   0.0   1.0\n",
       "2017 Bugatti Veyron  1500.0    2.0   1.0   NaN"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars_dict = {'2008 Honda Civic':    {'hp':150., 'seats':5., 'auto':0., 'cool':0}, \n",
    "             '2011 Dodge Charger' : {'hp':320., 'seats':5., 'auto':0., 'cool':1}, \n",
    "            '1985 Chevy Blazer':    {'hp':383., 'seats':3., 'auto':1., 'cool':1}, \n",
    "             '2001 Honda Odyssey':  {'hp':210., 'seats':7., 'auto':0., 'cool':0}, \n",
    "             '2017 Bugatti Veyron': {'hp':1500.,'seats':2., 'auto':1., 'cool':None}}\n",
    "\n",
    "data = pd.DataFrame.from_dict(cars_dict,orient='index')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5STDuS1DF0PA"
   },
   "source": [
    "### Normalizing Data and Calculating Distance\n",
    "\n",
    "In order to tell how \"near\" one datum is to another, we need a way to measure the distance between two data. One of the simplest ways to do this (and the way we will be doing it) is with simple Euclidian distance (formula below). Euclidean distance is the square root of the sum of the difference between each feature squared:\n",
    "\n",
    "![alt text](https://i.stack.imgur.com/2y0bx.png.)\n",
    "\n",
    "Some other approaches include Chi square distance and cosine distance (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4978658/). \n",
    "\n",
    "NOTE: By normalizing the data in this way we are assuming all features are equally important, but there are ways to weight some features more/less than others.\n",
    "\n",
    "If we simply compute the Euclidean distance from one datum to another, values that are generally larger (like horsepower) will end up having a greater effect than the other smaller values. Because features with large values are not inherently more important for prediction than other values, we should normalize the data before we calculate distance. One quick and easy way to normalize data is to divide each datum by the maximum value in its category (i.e. divide each element in a row by the max value in that row).\n",
    "\n",
    "In the cells below, fill in the code to normalize the data and create a function that will calculate the euclidean distance between two cars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1531158434256,
     "user": {
      "displayName": "Danelle Shah",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "100860318553187717982"
     },
     "user_tz": 240
    },
    "id": "aBhWCPsFMn-C",
    "outputId": "b62e3aac-6ddf-4158-abf1-b7e23e4e9c75"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hp</th>\n",
       "      <th>seats</th>\n",
       "      <th>auto</th>\n",
       "      <th>cool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1985 Chevy Blazer</th>\n",
       "      <td>0.255333</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001 Honda Odyssey</th>\n",
       "      <td>0.140000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008 Honda Civic</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011 Dodge Charger</th>\n",
       "      <td>0.213333</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017 Bugatti Veyron</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           hp     seats  auto  cool\n",
       "1985 Chevy Blazer    0.255333  0.428571   1.0   1.0\n",
       "2001 Honda Odyssey   0.140000  1.000000   0.0   0.0\n",
       "2008 Honda Civic     0.100000  0.714286   0.0   0.0\n",
       "2011 Dodge Charger   0.213333  0.714286   0.0   1.0\n",
       "2017 Bugatti Veyron  1.000000  0.285714   1.0   NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the data by dividing each value by the maximum value in its row. \n",
    "# Do not normalize the labels indicating cool and uncool (row 3)\n",
    "\n",
    "for i in ['hp','seats','auto']:\n",
    "    data[i] = data[i]/max(data[i].values)\n",
    "  \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hp</th>\n",
       "      <th>seats</th>\n",
       "      <th>auto</th>\n",
       "      <th>cool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1985 Chevy Blazer</th>\n",
       "      <td>-0.259004</td>\n",
       "      <td>-0.802955</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001 Honda Odyssey</th>\n",
       "      <td>-0.604742</td>\n",
       "      <td>1.491202</td>\n",
       "      <td>-0.816497</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008 Honda Civic</th>\n",
       "      <td>-0.724651</td>\n",
       "      <td>0.344124</td>\n",
       "      <td>-0.816497</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011 Dodge Charger</th>\n",
       "      <td>-0.384908</td>\n",
       "      <td>0.344124</td>\n",
       "      <td>-0.816497</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017 Bugatti Veyron</th>\n",
       "      <td>1.973305</td>\n",
       "      <td>-1.376494</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           hp     seats      auto  cool\n",
       "1985 Chevy Blazer   -0.259004 -0.802955  1.224745   1.0\n",
       "2001 Honda Odyssey  -0.604742  1.491202 -0.816497   0.0\n",
       "2008 Honda Civic    -0.724651  0.344124 -0.816497   0.0\n",
       "2011 Dodge Charger  -0.384908  0.344124 -0.816497   1.0\n",
       "2017 Bugatti Veyron  1.973305 -1.376494  1.224745   NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize data by removing the mean and scaling to unit variance from each feature\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "data_unitnorm = pd.DataFrame.from_dict(cars_dict,orient='index')\n",
    "\n",
    "for i in ['hp','seats','auto']:\n",
    "    feature_data = data_unitnorm[i].values.reshape(-1, 1)\n",
    "    scaler.fit(feature_data)\n",
    "    data_unitnorm[i] = scaler.transform(feature_data)\n",
    "    \n",
    "data_unitnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "2rayoFQnOyvY"
   },
   "outputs": [],
   "source": [
    "# Distance Function using formula for euclidean distance\n",
    "\n",
    "def euclidean_dist(new_datum, other_datum):\n",
    "\n",
    "    inner_val = 0.0\n",
    "    \n",
    "    for g in range(new_datum.shape[0]):\n",
    "        inner_val += (new_datum[g]- other_datum[g]) ** 2\n",
    "    \n",
    "    distance = math.sqrt(inner_val)\n",
    "    return(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6IaktG5_mUNf"
   },
   "source": [
    "In the cell below, calculate the distance between the Bugatti and each other car using euclidean_dist. Remember not to use row three (cool/uncool label) when computing distance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "rY0Wuzz9LZzu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.28571429, 1.        ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is how you can call a specific row by name and sub-select features\n",
    "bugatti = data.loc[\"2017 Bugatti Veyron\"][[\"hp\",\"seats\",\"auto\"]].values\n",
    "bugatti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1531159541109,
     "user": {
      "displayName": "Danelle Shah",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "100860318553187717982"
     },
     "user_tz": 240
    },
    "id": "ov8bkRifM8a2",
    "outputId": "da48e595-bc48-4035-de49-6ed8c306a016"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Distances to 2017 Bugatti Veyron (V1)\n",
      "  1985 Chevy Blazer: \t0.758\n",
      "  2001 Honda Odyssey: \t1.500\n",
      "  2008 Honda Civic: \t1.412\n",
      "  2011 Dodge Charger: \t1.343\n",
      "  2017 Bugatti Veyron: \t0.000\n",
      "\n",
      "Euclidean Distances to 2017 Bugatti Veyron (V2)\n",
      "  1985 Chevy Blazer: \t2.305\n",
      "  2001 Honda Odyssey: \t4.363\n",
      "  2008 Honda Civic: \t3.796\n",
      "  2011 Dodge Charger: \t3.562\n",
      "  2017 Bugatti Veyron: \t0.000\n"
     ]
    }
   ],
   "source": [
    "print('Euclidean Distances to 2017 Bugatti Veyron (V1)')\n",
    "for car in list(data.index):\n",
    "    d = euclidean_dist(bugatti, \n",
    "                       data.loc[\"{}\".format(car)][[\"hp\",\"seats\",\"auto\"]].values)\n",
    "    print('  {}: \\t{:01.3f}'.format(car,d))\n",
    "    \n",
    "print('\\nEuclidean Distances to 2017 Bugatti Veyron (V2)')\n",
    "for car in list(data_unitnorm.index):\n",
    "    d = euclidean_dist(data_unitnorm.loc[\"2017 Bugatti Veyron\"][[\"hp\",\"seats\",\"auto\"]].values, \n",
    "                       data_unitnorm.loc[\"{}\".format(car)][[\"hp\",\"seats\",\"auto\"]].values)\n",
    "    print('  {}: \\t{:01.3f}'.format(car,d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rHRn4t68nIbB"
   },
   "source": [
    "You should get the following distances (rounded to the thousandths palce):\n",
    "\n",
    "Bugatti - Blazer = 0.758\n",
    "\n",
    "Bugatti - Odyssey = 1.500\n",
    "\n",
    "Bugatti - Civic = 1.412\n",
    "\n",
    "Bugatti - Charger = 1.343\n",
    "\n",
    "Because the distance between the Bugatti and Blazer is the smallest, if k = 1, we would classify the Bugatti as cool. If k = 4 we would not be able to classify the Bugatti in either category using the \"majority vote\" technique unless we had in place a tiebreaker protocol. Generally speaking, larger values of *k* reduce noise, but also make the boundaries between classes less distinct. The best value of *k* will depend on your dataset and your prediction needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vo5iwH6soijh"
   },
   "source": [
    "# Predicting Diabetes in Pima Heritage Dataset\n",
    "\n",
    "Next we will see if we can use k-NNA to predict whether or not a patient has diabetes given some medical information about them. Load and view the data in the cell below. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 402,
     "status": "ok",
     "timestamp": 1531159718063,
     "user": {
      "displayName": "Danelle Shah",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "100860318553187717982"
     },
     "user_tz": 240
    },
    "id": "hQAjk52WQ1da",
    "outputId": "14b27fa3-9a09-4762-aa2e-bcb576400d6c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>test</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   preg  plas  pres  skin  test  mass   pedi  age  class\n",
       "0     6   148    72    35     0  33.6  0.627   50      1\n",
       "1     1    85    66    29     0  26.6  0.351   31      0\n",
       "2     8   183    64     0     0  23.3  0.672   32      1\n",
       "3     1    89    66    23    94  28.1  0.167   21      0\n",
       "4     0   137    40    35   168  43.1  2.288   33      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Loading and preprocessing\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "data = pd.read_csv(url, names=names)\n",
    "\n",
    "#  'preg': number of pregnancies  \n",
    "#  'plas': plasma glucose concentration \n",
    "#  'pres': blood pressure \n",
    "#  'skin': skin thickness\n",
    "#  'test': Insulin\n",
    "#  'mass': BMI\n",
    "#  'pedi': diabetes pedigree function\n",
    "#  'age': age\n",
    "#  'class': '0' means does not have diabetes and '1' means has diabetes\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lO_knzAAe93n"
   },
   "source": [
    "Now, let's clearly define which columns will act as explanatory variables, and which column will be the target value, and split the dataset between your training data and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1531160088058,
     "user": {
      "displayName": "Danelle Shah",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "100860318553187717982"
     },
     "user_tz": 240
    },
    "id": "b8pRTCC3PatJ",
    "outputId": "47ceb8c1-e308-4974-ba9f-91587232c917"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(614, 8) (614,)\n",
      "(154, 8) (154,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# columns we will use to make predictions with\n",
    "x_cols = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age']\n",
    "\n",
    "# column that we want to predict\n",
    "y_col = 'class'\n",
    "\n",
    "# 80-20 split of dataset\n",
    "test_size = 0.2\n",
    "x_training, x_testing, y_training, y_testing = train_test_split(data[x_cols], data[y_col], test_size=test_size, random_state=0)\n",
    "\n",
    "print(x_training.shape, y_training.shape)\n",
    "print(x_testing.shape, y_testing.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's not forget to normalize the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StandardScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a1b9a4a9ddd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_training\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mfeature_data_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_training\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mfeature_data_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_testing\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StandardScaler' is not defined"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "for i in list(x_training):\n",
    "    feature_data_train = x_training[i].values.reshape(-1, 1)\n",
    "    feature_data_test = x_testing[i].values.reshape(-1, 1)\n",
    "    \n",
    "    # normalize only according to training data!\n",
    "    scaler.fit(feature_data_train)\n",
    "    \n",
    "    x_training[i] = scaler.transform(feature_data_train)\n",
    "    x_testing[i] = scaler.transform(feature_data_test)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzZQ1VhiPSJU"
   },
   "source": [
    "This predict method needs to compute the euclidean distance between the “new” observation and all the data points in the training set. Then, it assigns the corresponding label to the observation. Finally, it selects the K nearest ones and performs a \"majority vote.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "UCganwZfe9LA"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def predict(x_training, y_training, x_test_sample, k):\n",
    "    \n",
    "    # create list for distances and targets\n",
    "    distances = []\n",
    "    targets = []\n",
    "\n",
    "    # use our previously made euclidean distance function, calculate distance to all samples in training set\n",
    "    for q in list(x_training.index):\n",
    "        distances.append([euclidean_dist(x_test_sample, x_training.loc[q]), q])\n",
    "  \n",
    "    # sort distances (smallest to largest)\n",
    "    distances = sorted(distances)\n",
    "  \n",
    "    # make a list of the k neighbors' targets\n",
    "    for i in range(k):\n",
    "        dix = distances[i][1]\n",
    "        targets.append(y_training.loc[dix])\n",
    "        \n",
    "    # retrieve the most common target\n",
    "    c=Counter(targets)\n",
    "    return c.most_common()[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JNA504XOvjvY"
   },
   "outputs": [],
   "source": [
    "def knn(x_training, y_training, x_testing, k):\n",
    "    \n",
    "    predictions = np.empty([x_testing.shape[0],])\n",
    "    \n",
    "    # loop over all test samples\n",
    "    px = 0\n",
    "    for i in list(x_testing.index):\n",
    "        predictions[px]=predict(x_training, y_training, x_testing.loc[i], k)\n",
    "        px+=1\n",
    "        \n",
    "    return predictions.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "iew5gW_AbnfN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 48.11781144142151 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "predictions_slow = knn(x_training, y_training, x_testing, k=5)\n",
    "\n",
    "print('Took {} seconds'.format(time.time() - start))\n",
    "predictions_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7-2ZM8AGIe0S"
   },
   "source": [
    "# Using sklearn to speed up normalizing and distance finding\n",
    "\n",
    "Luckily for us, sklearn has some quick and easy functions for normalizing the data, finding Euclidean distances, training, and testing with k-NNA. Try k = 5 to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 13073
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1530526208018,
     "user": {
      "displayName": "Lyle Lalunio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "111684296322842137827"
     },
     "user_tz": 240
    },
    "id": "R0FsKkp_qw-x",
    "outputId": "39c27885-10d9-4f03-9bff-ac7c41d5dcb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.003000497817993164 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create Model with k nearest neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train/fit model with training data\n",
    "knn.fit(x_training, y_training)\n",
    "\n",
    "# Make predictions on the test data using the fitted model\n",
    "start = time.time()\n",
    "predictions_fast = knn.predict(x_testing)\n",
    "\n",
    "print('Took {} seconds'.format(time.time() - start))\n",
    "predictions_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating classification performance\n",
    "Let's see how well our classifier did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[93 14]\n",
      " [17 30]]\n",
      "\n",
      "F1:  0.6593406593406593\n",
      "\n",
      "TN:  93 \n",
      "FP:  14 \n",
      "FN:  17 \n",
      "TP:  30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "cm = confusion_matrix(y_testing, predictions_fast)\n",
    "\n",
    "print('Confusion Matrix: \\n',cm)\n",
    "print('\\nF1: ',f1_score(y_testing, predictions_fast,labels=[0,1]))\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print('\\nTN: ',tn, '\\nFP: ',fp, '\\nFN: ',fn, '\\nTP: ',tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F3dhm8i3Q6nh"
   },
   "source": [
    "You should get F1 = 0.659. Notice that this classifier is pretty good at classifying negative samples (compare TN to FP), but we are not very good at classifying positive samples (compare TP to FN).\n",
    "\n",
    "Spend a few minutes trying to increase the F1 score as much as you can by changing k and the features of the data you are using to predict values.\n",
    "\n",
    "* What set of features and values of k did you find to be the most optimal?\n",
    "* Why is choosing the right features so important for prediction accuracy? \n",
    "* What other model hyperparameters (other than k) might we be able to tune?\n",
    "* How might we use non-numerical data columns in our model (if we had any)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l_noD6IVArqt"
   },
   "source": [
    "# Pros and Cons of k-NNA\n",
    "\n",
    "## Pros\n",
    "* Non-parametric (can be used with data that does not fit a normal distribution)\n",
    "* Conceptually simple and relatively simple to instantiate\n",
    "* Little to no \"training\" time \n",
    "* A good starting point/baseline classifier \n",
    "\n",
    "## Cons\n",
    "* Slow \"testing\" phase compared to other predictors/classifiers \n",
    "* Degrades with high-dimension data (because there is less difference between closest and furthest neighbors)\n",
    "* Unclear how to handle non-numeric features\n",
    "* Doesn't handle data with skewed class distribution well (if one class is extremely dominant in the training data, it will dominate the \"voting majority\" for classifying new data)\n",
    "* Features that are not of the same scale can (should) be normalized, but this introduces another model hyperparameter (i.e., which normalization method is appropriate?)\n",
    "* Does not take into account feature correlation\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "From these exercises and the pros and cons listed above, here is when it is most useful to use k-NNA:\n",
    "* Datasets with many data points and few dimensions (but can become very slow as well)\n",
    "* Datasets that are non-parametric\n",
    "* When you want a quick and easy classifier that does not have to be optimal (perhaps to use as a baseline for other models)\n",
    "\n",
    "\n",
    "Lastly, there are many other types of NNAs and there are many other ways to instantiate the k-NNA. For example, we could have used a weighted voting system (nearer neighbors' votes carry more weight) instead of a majority voting system to classify. Additionally, we could have used a different distance measurement or a different error measurement. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Nearest_Neighbors_Solutions.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
